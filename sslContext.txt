import pytest
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from unittest.mock import patch, MagicMock

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local").appName("TestSession").getOrCreate()

@pytest.fixture
def input_dataframes(spark):
    # Load input CSVs
    job_signoff_df = spark.read.options(header=True, delimiter=".").csv("tests/unit/inputfiles/job_signoff.csv")
    job_inst_df = spark.read.options(header=True, delimiter=".").csv("tests/unit/inputfiles/job_inst_cib_invoice.csv")
    fe_cib_invoice_raw_df = spark.read.options(header=True, delimiter=".").csv("tests/unit/inputfiles/fe_cib_invoice_raw.csv")
    hrchy_entity_inst_fe_df = spark.read.options(header=True, delimiter=".").csv("tests/unit/inputfiles/hrchy_entity_inst_fe.csv")

    return {
        "job_signoff_df": job_signoff_df,
        "job_inst_df": job_inst_df,
        "fe_cib_invoice_raw_df": fe_cib_invoice_raw_df,
        "hrchy_entity_inst_fe_df": hrchy_entity_inst_fe_df
    }

# Manually add PROCESSING_MED to match the real implementation
    src_dataframe = src_dataframe.withColumn("PROCESSING_MED", lit("some_value"))

    # Perform join as in the original code
    df_joined_src_ccy = src_dataframe.alias('src').join(
        hie_dataframe_ccy_whr.alias('hie_ccy'),
        on=col('src.SUB_FUND_CCY') == col('hie_ccy.ENTITY_NM'),
        how="left"
    ).select(
        col('src.PROCESSING_MED'), col('src.TW04_INVOICE_SECTION'),
        col('hie_ccy.ENTITY_CD').alias('ccy_Ref_Hie_Cd')
    )

def test_dataframe_join(spark, input_dataframes):
    # Extract input DataFrames
    src_dataframe = input_dataframes["fe_cib_invoice_raw_df"]
    hie_dataframe_ccy_whr = input_dataframes["hrchy_entity_inst_fe_df"]

    # Perform join as in the original code
    df_joined_src_ccy = src_dataframe.alias('src').join(
        hie_dataframe_ccy_whr.alias('hie_ccy'),
        on=col('src.SUB_FUND_CCY') == col('hie_ccy.ENTITY_NM'),
        how="left"
    ).select(
        col('src.PROCESSING_MED'), col('src.TW04_INVOICE_SECTION'),
        col('hie_ccy.ENTITY_CD').alias('ccy_Ref_Hie_Cd')
    )

    # Assert output is not empty
    assert df_joined_src_ccy.count() > 0, "Join operation returned empty DataFrame"

    # Validate expected columns exist
    expected_columns = {'PROCESSING_MED', 'TW04_INVOICE_SECTION', 'ccy_Ref_Hie_Cd'}
    assert expected_columns.issubset(set(df_joined_src_ccy.columns)), "Missing expected columns"

def test_mock_join():
    with patch.object(SparkSession, 'read', return_value=MagicMock()) as mock_read:
        fe_cib_invoice_raw_df = mock_read.csv("tests/unit/inputfiles/fe_cib_invoice_raw.csv")

        with patch.object(fe_cib_invoice_raw_df, 'join', return_value=MagicMock()) as mock_join:
            result = fe_cib_invoice_raw_df.join(MagicMock(), on="SUB_FUND_CCY", how="left")
            assert mock_join.called, "Join was not executed"
from unittest.mock import patch, MagicMock
from pyspark.sql import SparkSession, Row
from pyspark.sql.types import StructType, StructField, StringType
import pytest

@pytest.mark.fasttest
@patch('pyspark.sql.SparkSession.table')  # Mock table read
def test_fe_cib_invoice_transformer(mock_table, spark):
    # Mock the Spark session
    mock_spark = MagicMock()
    
    # Define schema for hrchy_entity_inst_fe
    hrchy_entity_inst_fe_schema = StructType([
        StructField("ENTITY_CLASS", StringType(), True),
        StructField("ENTITY_NM", StringType(), True),
        StructField("ENTITY_CD", StringType(), True)  # Ensure this column exists
    ])

    # Create mock DataFrame for hrchy_entity_inst_fe
    hrchy_entity_inst_fe_df = spark.createDataFrame([
        Row(ENTITY_CLASS="fe_cib_ccy", ENTITY_NM="USD", ENTITY_CD="USD123")
    ], hrchy_entity_inst_fe_schema)

    # Set up mock behavior for `spark.read.table()`
    mock_table.return_value = hrchy_entity_inst_fe_df

    # Simulate reading from table in your code
    hie_dataframe = spark.read.table("25591_ctg_dev.fdi_refined_amfin_fe_schema.hrchy_entity_inst_fe")

    # Validate that the mock is used
    assert hie_dataframe.count() > 0, "Mocked table dataframe is empty"

    # Join operation (simulating actual transformation logic)
    fe_cib_invoice_raw_schema = StructType([
        StructField("SUB_FUND_CCY", StringType(), True),
        StructField("PROCESSING_MED", StringType(), True),
        StructField("TW04_INVOICE_SECTION", StringType(), True)
    ])
    
    fe_cib_invoice_raw_df = spark.createDataFrame([
        Row(SUB_FUND_CCY="USD", PROCESSING_MED="MED_TYPE", TW04_INVOICE_SECTION="SECTION_1")
    ], fe_cib_invoice_raw_schema)

    df_joined_src_ccy = fe_cib_invoice_raw_df.alias('src').join(
        hie_dataframe.alias('hie_ccy'),
        on=fe_cib_invoice_raw_df["SUB_FUND_CCY"] == hie_dataframe["ENTITY_NM"],
        how="left"
    ).select(
        fe_cib_invoice_raw_df["PROCESSING_MED"],
        fe_cib_invoice_raw_df["TW04_INVOICE_SECTION"],
        hie_dataframe["ENTITY_CD"].alias("ccy_Ref_Hie_Cd")
    )

    # Validate join result
    assert df_joined_src_ccy.count() > 0, "Join operation returned empty DataFrame"

    # Validate expected columns
    expected_columns = {"PROCESSING_MED", "TW04_INVOICE_SECTION", "ccy_Ref_Hie_Cd"}
    assert expected_columns.issubset(set(df_joined_src_ccy.columns)), "Missing expected columns"

