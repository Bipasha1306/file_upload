import pytest
from unittest.mock import patch, MagicMock
from pyspark.sql import SparkSession, Row
from pyspark.sql.functions import col, lit
from pyspark.sql.types import StructType, StructField, StringType, DateType
from everest_pipeline_sdk.base.pipeline_results import PipelineResult, PipelineDataSetResult
from src.amimafinanceaws_everest2_transformation_derivation_fe_cib.transformations.fe_cib_invoice_trnsfmd import feCibInvoiceLoadTransformer
import datetime

@pytest.mark.fasttest
@patch('src.amimafinanceaws_everest2_transformation_derivation_fe_cib.transformations.fe_cib_invoice_trnsfmd.SparkSession.getActiveSession')
@patch('pyspark.sql.SparkSession.table')
def test_fe_cib_invoice_transformer(mock_table, mock_get_active_session):
    # Create a mock Spark session
    mock_spark = MagicMock()
    mock_get_active_session.return_value = mock_spark

    # Define your variables
    catlog = 'test_catalog'
    c_co_loadid_str = '12345'
    row_loadid_count = 1

    # Your SQL strings
    str_upd_sts_active = (
        "UPDATE {ctg}.fdi_refined_amfin_ops_schema.job_inst SET JOB_STATUS = 'FLS_DBX_ACTIVE' "
        "WHERE JOB_ID = {loadid_var}".format(
            ctg=catlog, loadid_var=c_co_loadid_str)
    )

    str_del_sql = (
        "DELETE FROM {ctg}.fdi_refined_amfin_ops_schema.job_errors WHERE JOB_ID == {loadid_var}"
        .format(ctg=catlog, loadid_var=c_co_loadid_str)
    )

    str_del_srcdata_sql = (
        "UPDATE {ctg}.fdi_refined_amfin_fe_schema.fe_cib_invoice_trnsfmd SET ACTIVE_FLAG = 'N' "
        "WHERE JOB_ID = {loadid_var}".format(
            ctg=catlog, loadid_var=c_co_loadid_str)
    )

    # Create a Spark session for testing
    spark = SparkSession.builder.master("local").appName("test").getOrCreate()

    # Create mock DataFrames with the necessary columns
    job_signoff_schema = StructType([
        StructField("REPORTING_DATE", DateType(), True)
    ])
    job_inst_schema = StructType([
        StructField("REPORTING_DT", StringType(), True),
        StructField("JOB_STATUS", StringType(), True),
        StructField("JOB_ID", StringType(), True),
        StructField("JOB_CD", StringType(), True)
    ])
    fe_cib_invoice_raw_schema = StructType([
        StructField("SUB_FUND_CCY", StringType(), True),
        StructField("PROCESSING_MED", StringType(), True),
        StructField("TW04_INVOICE_SECTION", StringType(), True)
    ])
    hrchy_entity_inst_fe_schema = StructType([
        StructField("ENTITY_CLASS", StringType(), True),
        StructField("ENTITY_NM", StringType(), True),
        StructField("ENTITY_CD", StringType(), True)
    ])

    job_signoff_df = spark.createDataFrame(
        [Row(REPORTING_DATE=datetime.date(2025, 3, 1))], job_signoff_schema)
    job_inst_df = spark.createDataFrame(
        [Row(REPORTING_DT='2025-03-01', JOB_STATUS='FLS_BYODSUCCESS', JOB_ID='10008', JOB_CD='SRC_FE_CIB_INVOICE')], job_inst_schema)
    fe_cib_invoice_raw_df = spark.createDataFrame(
        [Row(SUB_FUND_CCY='USD', PROCESSING_MED='MED_TYPE', TW04_INVOICE_SECTION='SECTION_1')], fe_cib_invoice_raw_schema)

    # Read input files
    job_signoff_df = spark.read.options(header=True).options(delimiter=",").csv("tests/unit/inputfiles/job_signoff.csv")
    job_inst_df = spark.read.options(header=True).options(delimiter=",").csv("tests/unit/inputfiles/job_inst_cib_invoice.csv")
    fe_cib_invoice_raw_df = spark.read.options(header=True).options(delimiter=",").csv("tests/unit/inputfiles/fe_cib_invoice_raw.csv")

    # Ensure DataFrames are not empty
    assert job_signoff_df is not None, "job_signoff_df is not loaded"
    assert job_inst_df is not None, "job_inst_df is not loaded"
    assert fe_cib_invoice_raw_df is not None, "fe_cib_invoice_raw_df is not loaded"

    # Create a PipelineResult object with the mock data
    input_df = PipelineResult() \
        .add_data_set(PipelineDataSetResult('job_signoff_df').add_data(job_signoff_df)) \
        .add_data_set(PipelineDataSetResult('job_inst_df').add_data(job_inst_df)) \
        .add_data_set(PipelineDataSetResult('fe_cib_invoice_raw_df').add_data(fe_cib_invoice_raw_df))

    hrchy_entity_inst_fe_df = spark.createDataFrame([
        Row(ENTITY_CLASS="fe_cib_ccy", ENTITY_NM="USD", ENTITY_CD="USD123")
    ], hrchy_entity_inst_fe_schema)

    mock_table.return_value = hrchy_entity_inst_fe_df

    hie_dataframe = spark.table("25591_ctg_dev.fdi_refined_amfin_fe_schema.hrchy_entity_inst_fe")
    print("hie_dataframe preview:")
    hie_dataframe.show()

    assert hie_dataframe.count() > 0, "Hierarchy DataFrame is empty"

    # Additional logic to be added
    fe_cib_invoice_raw_schema = StructType([
        StructField("SUB_FUND_CCY", StringType(), True),
        StructField("PROCESSING_MED", StringType(), True),
        StructField("TW04_INVOICE_SECTION", StringType(), True)
    ])

    fe_cib_invoice_raw_df = spark.createDataFrame([
        Row(SUB_FUND_CCY="USD", PROCESSING_MED="MED_TYPE", TW04_INVOICE_SECTION="SECTION_1")
    ], fe_cib_invoice_raw_schema)

    df_joined_src_ccy = fe_cib_invoice_raw_df.alias('src').join(
        hie_dataframe.alias('hie_ccy'),
        on=fe_cib_invoice_raw_df["SUB_FUND_CCY"] == hie_dataframe["ENTITY_NM"],
        how="left"
    ).select(
        fe_cib_invoice_raw_df["PROCESSING_MED"],
        fe_cib_invoice_raw_df["TW04_INVOICE_SECTION"],
        hie_dataframe["ENTITY_CD"].alias("ccy_Ref_Hie_Cd")
    )

    print("df_joined_src_ccy preview:")
    df_joined_src_ccy.show()

    assert df_joined_src_ccy.count() > 0, "Join operation returned empty DataFrame"
    assert "ccy_Ref_Hie_Cd" in df_joined_src_ccy.columns, "Missing expected column 'ccy_Ref_Hie_Cd'"

    # Update input_df with the new fe_cib_invoice_raw_df
    input_df = PipelineResult() \
        .add_data_set(PipelineDataSetResult('fe_cib_invoice_raw_df').add_data(fe_cib_invoice_raw_df))

    transformer = feCibInvoiceLoadTransformer()
    result_df = transformer.execute_transformer(input_df, "25591_ctg_dev")

    # Ensure the transformation result is not None
    assert result_df is not None, "Transformation result is None"
    print("Result DataFrame preview:")
    result_df.show()

    # Read expected output file
    expected_output_df = spark.read.options(header=True).options(delimiter=",").csv("tests/unit/outputfiles/fe_cib_invoice_trnsfmd.csv")

    # Compare the final DataFrame with the expected output
    subtract_df = result_df.subtract(expected_output_df)
    assert subtract_df.count() == 0, "The final DataFrame does not match the expected output"

    # Simulate the logic
    if row_loadid_count == 1:
        # Execute SQL statements
        mock_spark.sql(str_upd_sts_active)
        mock_spark.sql(str_del_sql)
        mock_spark.sql(str_del_srcdata_sql)

    # Assert that the SQL methods were called with the correct queries
    mock_spark.sql.assert_any_call(str_upd_sts_active)
    mock_spark.sql.assert_any_call(str_del_sql)
    mock_spark.sql.assert_any_call(str_del_srcdata_sql)
