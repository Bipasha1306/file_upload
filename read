import pytest
from unittest.mock import patch, MagicMock
from pyspark.sql import SparkSession, Row
from pyspark.sql.functions import col
from pyspark.sql.types import StructType, StructField, StringType, DateType
from everest_pipeline_sdk.base.pipeline_results import PipelineResult, PipelineDataSetResult
from src.amimafinanceaws_everest2_transformation_derivation_fe_cib.transformations.fe_cib_invoice_trnsfmd import feCibInvoiceLoadTransformer
import datetime

@pytest.mark.fasttest
@patch('pyspark.sql.SparkSession.table')
@patch('src.amimafinanceaws_everest2_transformation_derivation_fe_cib.transformations.fe_cib_invoice_trnsfmd.SparkSession.getActiveSession')
def test_fe_cib_invoice_transformer(mock_get_active_session, mock_table, spark):
    mock_spark = MagicMock()
    mock_get_active_session.return_value = mock_spark
    
    catlog = 'test_catalog'
    c_co_loadid_str = '12345'
    row_loadid_count = 1
    
    str_upd_sts_active = (
        "UPDATE {ctg}.fdi_refined_amfin_ops_schema.job_inst SET JOB_STATUS = 'FLS_DBX_ACTIVE' "
        "WHERE JOB_ID = {loadid_var}".format(
            ctg=catlog, loadid_var=c_co_loadid_str)
    )
    
    str_del_sql = (
        "DELETE FROM {ctg}.fdi_refined_amfin_ops_schema.job_errors WHERE JOB_ID == {loadid_var}".format(
            ctg=catlog, loadid_var=c_co_loadid_str)
    )
    
    str_del_srcdata_sql = (
        "UPDATE {ctg}.fdi_refined_amfin_fe_schema.fe_cib_invoice_trnsfmd SET ACTIVE_FLAG = 'N' "
        "WHERE JOB_ID = {loadid_var}".format(
            ctg=catlog, loadid_var=c_co_loadid_str)
    )
    
    hrchy_entity_inst_fe_schema = StructType([
        StructField("ENTITY_CLASS", StringType(), True),
        StructField("ENTITY_NM", StringType(), True),
        StructField("ENTITY_CD", StringType(), True)
    ])
    
    hrchy_entity_inst_fe_df = spark.createDataFrame([
        Row(ENTITY_CLASS="fe_cib_ccy", ENTITY_NM="USD", ENTITY_CD="USD123")
    ], hrchy_entity_inst_fe_schema)
    
    mock_table.return_value = hrchy_entity_inst_fe_df
    
    hie_dataframe = spark.table("25591_ctg_dev.fdi_refined_amfin_fe_schema.hrchy_entity_inst_fe")
    print("hie_dataframe preview:")
    hie_dataframe.show()
    
    assert hie_dataframe.count() > 0, "Hierarchy DataFrame is empty"
    
    fe_cib_invoice_raw_schema = StructType([
        StructField("SUB_FUND_CCY", StringType(), True),
        StructField("PROCESSING_MED", StringType(), True),
        StructField("TW04_INVOICE_SECTION", StringType(), True)
    ])
    
    fe_cib_invoice_raw_df = spark.createDataFrame([
        Row(SUB_FUND_CCY="USD", PROCESSING_MED="MED_TYPE", TW04_INVOICE_SECTION="SECTION_1")
    ], fe_cib_invoice_raw_schema)
    
    df_joined_src_ccy = fe_cib_invoice_raw_df.alias('src').join(
        hie_dataframe.alias('hie_ccy'),
        on=fe_cib_invoice_raw_df["SUB_FUND_CCY"] == hie_dataframe["ENTITY_NM"],
        how="left"
    ).select(
        fe_cib_invoice_raw_df["PROCESSING_MED"],
        fe_cib_invoice_raw_df["TW04_INVOICE_SECTION"],
        hie_dataframe["ENTITY_CD"].alias("ccy_Ref_Hie_Cd")
    )
    
    print("df_joined_src_ccy preview:")
    df_joined_src_ccy.show()
    
    assert df_joined_src_ccy.count() > 0, "Join operation returned empty DataFrame"
    assert "ccy_Ref_Hie_Cd" in df_joined_src_ccy.columns, "Missing expected column 'ccy_Ref_Hie_Cd'"
    
    input_df = PipelineResult() \
        .add_data_set(PipelineDataSetResult('fe_cib_invoice_raw_df').add_data(fe_cib_invoice_raw_df))
    
    transformer = feCibInvoiceLoadTransformer()
    result_df = transformer.execute_transformer(input_df, "25591_ctg_dev")
    
    assert result_df is not None, "Transformation result is None"
    print("Result DataFrame preview:")
    result_df.show()
    
    expected_output_df = spark.read.options(header=True).options(delimiter=",")\
        .csv("tests/unit/outputfiles/fe_cib_invoice_trnsfmd.csv")
    
    subtract_df = result_df.subtract(expected_output_df)
    assert subtract_df.count() == 0, "The final DataFrame does not match the expected output"
    
    if row_loadid_count == 1:
        mock_spark.sql(str_upd_sts_active)
        mock_spark.sql(str_del_sql)
        mock_spark.sql(str_del_srcdata_sql)
    
    mock_spark.sql.assert_any_call(str_upd_sts_active)
    mock_spark.sql.assert_any_call(str_del_sql)
    mock_spark.sql.assert_any_call(str_del_srcdata_sql)
