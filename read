import pytest
from unittest.mock import patch, MagicMock
from pyspark.sql import SparkSession, Row
from pyspark.sql.types import StructType, StructField, StringType, DateType
from everest_pipeline_sdk.base.pipeline_results import PipelineResult, PipelineDataSetResult
from src.amimafinanceaws_everest2_transformation_derivation_fe_cib.transformations.fe_cib_invoice_trnsfmd import feCibInvoiceLoadTransformer
import datetime

@pytest.mark.fasttest
@patch('src.amimafinanceaws_everest2_transformation_derivation_fe_cib.transformations.fe_cib_invoice_trnsfmd.SparkSession.getActiveSession')
def test_fe_cib_invoice_transformer(mock_get_active_session):
    # Create a mock Spark session
    mock_spark = MagicMock()
    mock_get_active_session.return_value = mock_spark

    # Define variables
    catlog = 'test_catalog'
    c_co_loadid_str = '12345'
    row_loadid_count = 1

    # SQL statements
    str_upd_sts_active = (
        f"UPDATE {catlog}.fdi_refined_amfin_ops_schema.job_inst SET JOB_STATUS = 'FLS_DBX_ACTIVE' "
        f"WHERE JOB_ID = {c_co_loadid_str}"
    )

    str_del_sql = (
        f"DELETE FROM {catlog}.fdi_refined_amfin_ops_schema.job_errors WHERE JOB_ID = {c_co_loadid_str}"
    )

    str_del_srcdata_sql = (
        f"UPDATE {catlog}.fdi_refined_amfin_fe_schema.fe_cib_invoice_trnsfmd SET ACTIVE_FLAG = 'N' "
        f"WHERE JOB_ID = {c_co_loadid_str}"
    )

    # Define schema for job signoff
    job_signoff_schema = StructType([
        StructField("REPORTING_DATE", DateType(), True)
    ])
    
    # Mock DataFrame
    hrchy_entity_inst_fe_schema = StructType([
        StructField("ENTITY_CLASS", StringType(), True),
        StructField("ENTITY_NM", StringType(), True),
        StructField("ENTITY_CD", StringType(), True)
    ])
    
    hrchy_entity_inst_fe_df = mock_spark.createDataFrame([
        Row(ENTITY_CLASS="fe_cib_ccy", ENTITY_NM="USD", ENTITY_CD="USD123")
    ], hrchy_entity_inst_fe_schema)
    
    mock_spark.table.return_value = hrchy_entity_inst_fe_df
    
    hie_dataframe = mock_spark.table("25591_ctg_dev.fdi_refined_amfin_fe_schema.hrchy_entity_inst_fe")
    
    print("hie_dataframe preview:")
    hie_dataframe.show()
    assert hie_dataframe.count() > 0, "Hierarchy DataFrame is empty"

    fe_cib_invoice_raw_schema = StructType([
        StructField("SUB_FUND_CCY", StringType(), True),
        StructField("PROCESSING_MED", StringType(), True),
        StructField("TW04_INVOICE_SECTION", StringType(), True)
    ])

    fe_cib_invoice_raw_df = mock_spark.createDataFrame([
        Row(SUB_FUND_CCY="USD", PROCESSING_MED="MED_TYPE", TW04_INVOICE_SECTION="SECTION_1")
    ], fe_cib_invoice_raw_schema)

    df_joined_src_ccy = fe_cib_invoice_raw_df.alias('src').join(
        hie_dataframe.alias('hie_ccy'),
        on=fe_cib_invoice_raw_df["SUB_FUND_CCY"] == hie_dataframe["ENTITY_NM"],
        how="left"
    ).select(
        fe_cib_invoice_raw_df["PROCESSING_MED"],
        fe_cib_invoice_raw_df["TW04_INVOICE_SECTION"],
        hie_dataframe["ENTITY_CD"].alias("ccy_Ref_Hie_Cd")
    )
    
    print("df_joined_src_ccy preview:")
    df_joined_src_ccy.show()
    
    assert df_joined_src_ccy.count() > 0, "Join operation returned empty DataFrame"
    assert "ccy_Ref_Hie_Cd" in df_joined_src_ccy.columns, "Missing expected column 'ccy_Ref_Hie_Cd'"
    
    # Create PipelineResult object
    input_df = PipelineResult().add_data_set(PipelineDataSetResult('fe_cib_invoice_raw_df').add_data(fe_cib_invoice_raw_df))

    transformer = feCibInvoiceLoadTransformer()
    result_df = transformer.execute_transformer(input_df, "25591_ctg_dev")
    
    assert result_df is not None, "Transformation result is None"
    print("Result DataFrame preview:")
    result_df.show()

    # Simulated SQL Execution
    if row_loadid_count == 1:
        mock_spark.sql.return_value = MagicMock()  # Prevent NoneType issues
        mock_spark.sql(str_upd_sts_active)
        mock_spark.sql(str_del_sql)
        mock_spark.sql(str_del_srcdata_sql)
        
    mock_spark.sql.assert_any_call(str_upd_sts_active)
    mock_spark.sql.assert_any_call(str_del_sql)
    mock_spark.sql.assert_any_call(str_del_srcdata_sql)
